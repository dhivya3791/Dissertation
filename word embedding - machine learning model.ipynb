{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose brand based dataset to know about Machine Learning model\n",
      "choose the below options\n",
      "\n",
      " 1.adiads Brand reviews \n",
      " 2.skechers Brand reviews \n",
      " 3.crocs Brand Reviews\n",
      "Enter number of your choice : 3\n",
      "\n",
      "**************************************************************************\n",
      "To run the particular classifier model\n",
      "choose the below options\n",
      "\n",
      " 1.Random Forest classifier         \n",
      " 2.Logistic Regression classifier         \n",
      " 3.Decision tree classifier         \n",
      " 4.Naive Bayes\n",
      "Enter number of your choice : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ndhiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing the data\n",
      "[(0, 1129), (1, 1154), (2, 10756)]\n",
      "After balancing the data\n",
      "class 0 --> neutral\n",
      " class 1 --> negative \n",
      " class 2 --> positive\n",
      "[(0, 10603), (1, 10613), (2, 10560)]\n",
      "the best hyperparameters: MultinomialNB(alpha=0.1)\n",
      "train accuracy: 0.3725767875125881\n",
      "test accuracy: 0.6843558282208589\n",
      "validation score: 0.3728285335006256\n",
      "********************************************\n",
      " classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.109     0.087     0.097       344\n",
      "           1      0.114     0.085     0.097       366\n",
      "           2      0.800     0.851     0.824      2550\n",
      "\n",
      "    accuracy                          0.684      3260\n",
      "   macro avg      0.341     0.341     0.340      3260\n",
      "weighted avg      0.650     0.684     0.666      3260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    ## importing the necessary library \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    ### reading the data as dataframe\n",
    "    df = pd.read_excel('shoes.xlsx',sheet_name=\"Sheet2\")\n",
    "\n",
    "    ### deleting the unwanted column\n",
    "    del(df[\"marketplace\"])\n",
    "    del(df[\"customer_id\"])\n",
    "    del(df[\"review_id\"])\n",
    "    del(df[\"product_parent\"])\n",
    "    del(df[\"vine\"])\n",
    "    del(df[\"review_headline\"])\n",
    "    del(df[\"total_votes\"])\n",
    "    del(df[\"product_category\"])\n",
    "\n",
    "    #### create a brand subset\n",
    "    df['product_title']=df['product_title'].apply(lambda x: x.lower())\n",
    "\n",
    "    conditions =[(df['product_title'].str.contains('adidas')),\n",
    "                (df['product_title'].str.contains('crocs')),\n",
    "                (df['product_title'].str.contains('skechers'))]\n",
    "    values=['adidas','crocs','skechers']\n",
    "    df['Brand']=np.select(conditions,values)\n",
    "\n",
    "\n",
    "    #### labeling the data using star rating\n",
    "    df[\"verified_purchase\"]=df.verified_purchase.map({'Y':1,'N':0})\n",
    "    df[\"positivity\"] = df[\"star_rating\"].apply(lambda x: 2 if x>3 else(0 if x==3 else 1))\n",
    "\n",
    "    ### #Text Cleaning\n",
    "    # 1.1 Define preprocess function\n",
    "    df[\"review_body\"] = df[\"review_body\"].astype(\"str\")\n",
    "    import string\n",
    "    import nltk\n",
    "    nltk.download('words')\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_stopwords = [\"i've\",\"i'm\",'on','ie','thesefor','im']\n",
    "    stopwords.extend(new_stopwords)\n",
    "    import re\n",
    "    wn=nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def removing_punc(ele):\n",
    "        # Convert the text into lowercase\n",
    "        ele = ele.lower()\n",
    "        #punctuation\n",
    "        ele = re.sub('[%s]' % re.escape(string.punctuation), '', ele)\n",
    "        # number\n",
    "        ele = re.sub(r'[0-9]', '', ele)\n",
    "        #new line\n",
    "        ele = re.sub('\\n', '', ele)\n",
    "        #white space\n",
    "        ele= re.sub(\"^\\s+\", \"\", ele)\n",
    "        return ele\n",
    "    df[\"review_body\"]=df[\"review_body\"].apply(lambda x: removing_punc(x))\n",
    "\n",
    "\n",
    "    def tokenize(txt):\n",
    "        \"\"\"tokenize each word by using split() function\"\"\"\n",
    "        tokens=re.split('\\W+', txt)\n",
    "        return tokens\n",
    "    df['tokenized_message']=df['review_body'].apply(lambda x: tokenize(x))\n",
    "\n",
    "    def clean_word(txt_tokenized):\n",
    "        \"\"\"removed the stopword and remove the numbers and get the base word using lemmatize function\"\"\"\n",
    "        new_word = [word for word in txt_tokenized if word not in stopwords]\n",
    "        new_word = [word for word in new_word if word.isalpha()]\n",
    "        new_word = [word for word in new_word if word in words]\n",
    "        new_word = [wn.lemmatize(word) for word in new_word]\n",
    "        return \" \".join(new_word)\n",
    "    df['st_cleaned_message']=df['tokenized_message'].apply(lambda x:clean_word(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def switch_fun(choice, t):\n",
    "    ''' based on brand of shoes the corresponding function calls'''\n",
    "    if choice == 1 and t == 1:\n",
    "        df=load_data()\n",
    "        x1,y1=adidas(df)\n",
    "        rf_trs,rf_ts,rf_vals = random_forest(x1,y1)\n",
    "    elif choice == 1 and t == 2:\n",
    "        df=load_data()\n",
    "        x1,y1=adidas(df)\n",
    "        lg_trs,lg_ts,lg_vals = logistic_regression(x1,y1)\n",
    "    elif choice == 1 and t == 3:\n",
    "        df=load_data()\n",
    "        x1,y1=adidas(df)\n",
    "        dt_trs,dt_ts,dt_vals = decision_tree(x1,y1)\n",
    "    elif choice == 1 and t == 4:\n",
    "        df=load_data()\n",
    "        x1,y1=adidas(df)\n",
    "        nb_trs,nb_ts,nb_vals = nb(x1,y1)\n",
    "    elif choice == 2 and t == 1:\n",
    "        df=load_data()\n",
    "        x1,y1=skechers(df)\n",
    "        srf_trs,srf_ts,srf_vals = random_forest(x1,y1)\n",
    "    elif choice == 2 and t == 2:\n",
    "        df=load_data()\n",
    "        x1,y1=skechers(df)\n",
    "        slg_trs,slg_ts,slg_vals = logistic_regression(x1,y1)\n",
    "    elif choice == 2 and t == 3:\n",
    "        df=load_data()\n",
    "        x1,y1=skechers(df)\n",
    "        sdt_trs,sdt_ts,sdt_vals = decision_tree(x1,y1)\n",
    "    elif choice == 2 and t == 4:\n",
    "        df=load_data()\n",
    "        x1,y1=skechers(df)\n",
    "        snb_trs,snb_ts,snb_vals = nb(x1,y1)\n",
    "    elif choice == 3 and t == 1:\n",
    "        df=load_data()\n",
    "        x1,y1=crocs(df)\n",
    "        crf_trs,crf_ts,crf_vals = random_forest(x1,y1)\n",
    "    elif choice == 3 and t == 2:\n",
    "        df=load_data()\n",
    "        x1,y1=crocs(df)\n",
    "        clg_trs,clg_ts,clg_vals = logistic_regression(x1,y1)\n",
    "    elif choice == 3 and t == 3:\n",
    "        df=load_data()\n",
    "        x1,y1=crocs(df)\n",
    "        cdt_trs,cdt_ts,cdt_vals = decision_tree(x1,y1)\n",
    "    elif choice == 3 and t == 4:\n",
    "        df=load_data()\n",
    "        x1,y1=crocs(df)\n",
    "        cnb_trs,cnb_ts,cnb_vals = nb(x1,y1)\n",
    "    \n",
    "    else: \n",
    "        unknown_action()\n",
    "    \n",
    "def unknown_action():\n",
    "    '''if the user input is wrong'''\n",
    "    print('invalid entry')\n",
    "    print('Please enter 1 or 2 or 3 for brand reviews')\n",
    "    print('Please enter 1 or 2 or 3 or 4 for machine learning model')\n",
    "    return 0\n",
    "\n",
    "\n",
    "def adidas(dataset):\n",
    "    ### extracting only the adidas shoe brand reviews from the dataset\n",
    "    adidas = dataset[dataset[\"Brand\"]==\"adidas\"].sort_values(by=[\"review_date\"], ascending=False)\n",
    "    \n",
    "    ### Removing unwanted column\n",
    "    del(adidas['product_title'])\n",
    "    del(adidas['review_body'])\n",
    "    del(adidas['Brand'])\n",
    "    del(adidas[\"review_date\"])\n",
    "    del(adidas['star_rating'])\n",
    "    del(adidas['tokenized_message'])\n",
    "\n",
    "    \n",
    "    ## define size of vocabulary and max length of the sentence in the review dataset\n",
    "    vocab_size = 4156\n",
    "    max_length = 291\n",
    "    \n",
    "    ## name the y variable as sentiment\n",
    "    import numpy as np\n",
    "    sentiment = np.array(adidas['positivity'])\n",
    "    sentiment\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import one_hot\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    ## one hot encoding for the words\n",
    "    encoded_reviews = [one_hot(d, vocab_size) for d in adidas['st_cleaned_message']]\n",
    "\n",
    "    ## padding \n",
    "    padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "\n",
    "    ## define X and y variable\n",
    "    x=padded_reviews\n",
    "    y=sentiment\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def skechers(dataset):\n",
    "    ### extracting only the adidas shoe brand reviews from the dataset\n",
    "    skechers = dataset[dataset[\"Brand\"]==\"skechers\"].sort_values(by=[\"review_date\"], ascending=False)\n",
    "    \n",
    "    ### Removing unwanted column\n",
    "    del(skechers['product_title'])\n",
    "    del(skechers['review_body'])\n",
    "    del(skechers['Brand'])\n",
    "    del(skechers[\"review_date\"])\n",
    "    del(skechers['star_rating'])\n",
    "    del(skechers['tokenized_message'])\n",
    "\n",
    "    \n",
    "    ## define size of vocabulary and max length of the sentence in the review dataset\n",
    "    vocab_size = 4800\n",
    "    max_length = 330\n",
    "    \n",
    "    ## name the y variable as sentiment\n",
    "    import numpy as np\n",
    "    sentiment = np.array(skechers['positivity'])\n",
    "    sentiment\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import one_hot\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    ## one hot encoding for the words\n",
    "    encoded_reviews = [one_hot(d, vocab_size) for d in skechers['st_cleaned_message']]\n",
    "\n",
    "    ## padding \n",
    "    padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "\n",
    "    ## define X and y variable\n",
    "    x=padded_reviews\n",
    "    y=sentiment\n",
    "    return x,y\n",
    "\n",
    "def crocs(dataset):\n",
    "    \n",
    "    ### extracting only the adidas shoe brand reviews from the dataset\n",
    "    crocs = dataset[dataset[\"Brand\"]==\"crocs\"].sort_values(by=[\"review_date\"], ascending=False)\n",
    "    \n",
    "    ### Removing unwanted column\n",
    "    del(crocs['product_title'])\n",
    "    del(crocs['review_body'])\n",
    "    del(crocs['Brand'])\n",
    "    del(crocs[\"review_date\"])\n",
    "    del(crocs['star_rating'])\n",
    "    del(crocs['tokenized_message'])\n",
    "\n",
    "    \n",
    "    ## define size of vocabulary and max length of the sentence in the review dataset\n",
    "    vocab_size = 5032\n",
    "    max_length = 235\n",
    "    \n",
    "    ## name the y variable as sentiment\n",
    "    import numpy as np\n",
    "    sentiment = np.array(crocs['positivity'])\n",
    "    sentiment\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import one_hot\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    ## one hot encoding for the words\n",
    "    encoded_reviews = [one_hot(d, vocab_size) for d in crocs['st_cleaned_message']]\n",
    "\n",
    "    ## padding \n",
    "    padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "\n",
    "    ## define X and y variable\n",
    "    x=padded_reviews\n",
    "    y=sentiment\n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def random_forest(x1,y1):\n",
    "    \n",
    "    # split the dataset as train and test for evalution\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.20, random_state = 0)\n",
    "    \n",
    "    from collections import Counter\n",
    "    print('Before balancing the data')\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    x_resampled, y_resampled = smote_tomek.fit_resample(x_train, y_train)\n",
    "    print('After balancing the data')\n",
    "    print('class 0 --> neutral\\n class 1 --> negative \\n class 2 --> positive')\n",
    "    print(sorted(Counter(y_resampled).items()))\n",
    "\n",
    "    \n",
    "    # RANDOM FOREST CLASSIFIER\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score,KFold,cross_validate,GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    random_grid = {'max_depth': [10, 20],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 'n_estimators': [ 15, 10]}\n",
    "\n",
    "    randomforest = GridSearchCV(RandomForestClassifier(),random_grid,cv=5)\n",
    "    randomforest_fit = randomforest.fit(x_resampled,y_resampled)\n",
    "\n",
    "\n",
    "    ##### best estimator\n",
    "    best_rf_model = randomforest_fit.best_estimator_\n",
    "    print('the best hyperparameters:',best_rf_model)\n",
    "\n",
    "    ##### cross validation score\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(best_rf_model, x_resampled, y_resampled, cv=kfold, scoring='accuracy')\n",
    "    rf_validation_score = scores.mean()\n",
    "\n",
    "    ### fit the model\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    best_rf_model.fit(x_resampled,y_resampled)\n",
    "\n",
    "    ### predict the model\n",
    "    y_pred_train = best_rf_model.predict(x_resampled)\n",
    "    y_pred_test = best_rf_model.predict(x_test)\n",
    "    rf_train_score = accuracy_score(y_pred_train,y_resampled)\n",
    "    rf_test_score = accuracy_score(y_pred_test,y_test)\n",
    "\n",
    "    print('train accuracy:',rf_train_score)\n",
    "    print('test accuracy:',rf_test_score)\n",
    "    print('validation score:',rf_validation_score)\n",
    "    print('********************************************')\n",
    "    print(' classification report:\\n',classification_report(y_pred_test,y_test,digits=3))\n",
    "    return rf_train_score,rf_test_score,rf_validation_score\n",
    "\n",
    "def logistic_regression(x1,y1):\n",
    "     # split the dataset as train and test for evalution\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.20, random_state = 0)\n",
    "    \n",
    "    from collections import Counter\n",
    "    print('Before balancing the data')\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    x_resampled, y_resampled = smote_tomek.fit_resample(x_train, y_train)\n",
    "    print('After balancing the data')\n",
    "    print('class 0 --> neutral\\n class 1 --> negative \\n class 2 --> positive')\n",
    "    print(sorted(Counter(y_resampled).items()))\n",
    "\n",
    "    ### LOGISTIC REGRESSION CLASSIFIER\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score,KFold,cross_validate,GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    ##### hyperparameter tuning\n",
    "    param_grid = {'C' : [0.1, 0.2, 0.3, 0.4, 0.5]}\n",
    "    logistic = GridSearchCV(LogisticRegression(),param_grid,cv=5)\n",
    "    logistic_fit = logistic.fit(x_resampled,y_resampled)\n",
    "\n",
    "\n",
    "    ##### best estimator\n",
    "    best_lg_model = logistic_fit.best_estimator_\n",
    "    print('the best hyperparameters:',best_lg_model)\n",
    "\n",
    "    #### cross validation score\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(best_lg_model, x_resampled, y_resampled, cv=kfold, scoring='accuracy')\n",
    "    lg_validation_score = scores.mean()\n",
    "\n",
    "    ### fit the model\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    best_lg_model.fit(x_resampled,y_resampled)\n",
    "\n",
    "    ### predict the model\n",
    "    y_pred_train = best_lg_model.predict(x_resampled)\n",
    "    y_pred_test = best_lg_model.predict(x_test)\n",
    "    lg_train_score = accuracy_score(y_pred_train,y_resampled)\n",
    "    lg_test_score = accuracy_score(y_pred_test,y_test)\n",
    "\n",
    "    print('train accuracy:',lg_train_score)\n",
    "    print('test accuracy:',lg_test_score)\n",
    "    print('validation score:',lg_validation_score)\n",
    "    print('********************************************')\n",
    "    print(' classification report:\\n',classification_report(y_pred_test,y_test,digits=3))\n",
    "    return lg_train_score,lg_test_score,lg_validation_score\n",
    "\n",
    "def decision_tree(x1,y1):\n",
    "     # split the dataset as train and test for evalution\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.20, random_state = 0) \n",
    "    \n",
    "    from collections import Counter\n",
    "    print('Before balancing the data')\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    x_resampled, y_resampled = smote_tomek.fit_resample(x_train, y_train)\n",
    "    print('After balancing the data')\n",
    "    print('class 0 --> neutral\\n class 1 --> negative \\n class 2 --> positive')\n",
    "    print(sorted(Counter(y_resampled).items()))\n",
    "    \n",
    "    \n",
    "    ## DECISION TREE CLASSIFIER\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score,KFold,cross_val_predict,cross_validate,GridSearchCV\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    ##### hyperparameter tuning\n",
    "    tree_para = {'criterion':['gini','entropy'],'max_depth':[1,2,3,4,5,20,30,40,50,70,90,120,150]}\n",
    "    decision_tree = GridSearchCV(DecisionTreeClassifier(), tree_para,scoring='accuracy')\n",
    "    decision_tree_fit =decision_tree.fit(x_resampled,y_resampled)\n",
    "\n",
    "    ##### best estimator\n",
    "    best_dt_model = decision_tree_fit.best_estimator_\n",
    "    print('the best hyperparameters:',best_dt_model)\n",
    "\n",
    "    #### cross validation score\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(best_dt_model, x_resampled, y_resampled, cv=kfold, scoring='accuracy')\n",
    "    dt_validation_score = scores.mean()\n",
    "\n",
    "    ### fit the model\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    best_dt_model.fit(x_resampled,y_resampled)\n",
    "\n",
    "    ### predict the model\n",
    "    y_pred_train = best_dt_model.predict(x_resampled)\n",
    "    y_pred_test = best_dt_model.predict(x_test)\n",
    "    dt_train_score = accuracy_score(y_pred_train,y_resampled)\n",
    "    dt_test_score = accuracy_score(y_pred_test,y_test)\n",
    "\n",
    "    print('train accuracy:',dt_train_score)\n",
    "    print('test accuracy:',dt_test_score)\n",
    "    print('validation score:',dt_validation_score)\n",
    "    print('********************************************')\n",
    "    print(' classification report:\\n',classification_report(y_pred_test,y_test,digits=3))\n",
    "    return dt_train_score,dt_test_score,dt_validation_score\n",
    "\n",
    "def nb(x1,y1):\n",
    "     # split the dataset as train and test for evalution\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.20, random_state = 0)\n",
    "    \n",
    "    from collections import Counter\n",
    "    print('Before balancing the data')\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    x_resampled, y_resampled = smote_tomek.fit_resample(x_train, y_train)\n",
    "    print('After balancing the data')\n",
    "    print('class 0 --> neutral\\n class 1 --> negative \\n class 2 --> positive')\n",
    "    print(sorted(Counter(y_resampled).items()))\n",
    "    \n",
    "    ###NAIVE BAYES\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score,KFold,cross_validate,GridSearchCV\n",
    "    from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "    from sklearn.metrics import classification_report\n",
    "    import numpy as np\n",
    "\n",
    "    params_NB = {'alpha': (0.1,0.2)}\n",
    "\n",
    "    nb = GridSearchCV(MultinomialNB(),params_NB,scoring='accuracy')\n",
    "    nb_fit = nb.fit(x_resampled,y_resampled)\n",
    "\n",
    "    ##### best estimator\n",
    "    best_nb_model = nb_fit.best_estimator_\n",
    "    print('the best hyperparameters:',best_nb_model)\n",
    "\n",
    "    ### cross validation score\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(best_nb_model, x_resampled, y_resampled, cv=kfold, scoring='accuracy')\n",
    "    nb_validation_score = scores.mean()\n",
    "\n",
    "    ### fit the model\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    best_nb_model.fit(x_resampled,y_resampled)\n",
    "\n",
    "    ### predict the model\n",
    "    y_pred_train = best_nb_model.predict(x_resampled)\n",
    "    y_pred_test = best_nb_model.predict(x_test)\n",
    "    nb_train_score = accuracy_score(y_pred_train,y_resampled)\n",
    "    nb_test_score = accuracy_score(y_pred_test,y_test)\n",
    "\n",
    "    print('train accuracy:',nb_train_score)\n",
    "    print('test accuracy:',nb_test_score)\n",
    "    print('validation score:',nb_validation_score)\n",
    "    print('********************************************')\n",
    "    print(' classification report:\\n',classification_report(y_pred_test,y_test,digits=3))\n",
    "    return nb_train_score,nb_test_score,nb_validation_score\n",
    "    \n",
    "        \n",
    "print('Choose brand based dataset to know about Machine Learning model')\n",
    "print('choose the below options')\n",
    "print('\\n 1.adiads Brand reviews \\n 2.skechers Brand reviews \\n 3.crocs Brand Reviews')\n",
    "try:\n",
    "    r = int(input(\"Enter number of your choice : \")) \n",
    "    \n",
    "except TypeError:\n",
    "        print(\"TypeError\")\n",
    "except:\n",
    "    print('invalid entry')\n",
    "    \n",
    "print('\\n**************************************************************************')\n",
    "print('To run the particular classifier model')\n",
    "print('choose the below options')\n",
    "print('\\n 1.Random Forest classifier \\\n",
    "        \\n 2.Logistic Regression classifier \\\n",
    "        \\n 3.Decision tree classifier \\\n",
    "        \\n 4.Naive Bayes')\n",
    "try:\n",
    "    m = int(input(\"Enter number of your choice : \")) \n",
    "    \n",
    "except TypeError:\n",
    "        print(\"TypeError\")\n",
    "except:\n",
    "    print('invalid entry')\n",
    "switch_fun(r,m)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
